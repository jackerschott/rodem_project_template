# @package _global_

# Order indicates overwriting
defaults:
  - _self_
  - trainer: default.yaml
  - model: default.yaml
  - datamodule: celeba.yaml
  - loggers: default.yaml
  - hydra: default.yaml
  - paths: default.yaml
  - callbacks: default.yaml
  - experiment: null

seed: 42 # For reproducibility
project_name: generation # Determines output directory path and wandb project
network_name: network1 # Used for both saving and wandb
ckpt_path: null  # Checkpoint path to resume training

# Extra tweaks available with the new pytorch version
precision: medium # Should use medium if on ampere gpus
compile: null # Can set to default for faster compiles (very buggy!!!)
tags: [] # Used for the loggers

# COMPLETELY replaces the above config with what is contained in ${paths.full_path}
# This is ideal for resuming a job, log to the same directory
# Will also resume the loggers and set the ckpt_path to the latest
full_resume: False
